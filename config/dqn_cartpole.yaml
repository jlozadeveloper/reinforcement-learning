# Configuraci√≥n de DQN
# input_shape: (4,)
# num_actions: 2
environment:
  id: "CartPole-v1"
  max_episode_steps: 4000
  render_mode: 'rgb_array'
  # reward_threshold: 995
model:
  classname: "mpl"
  dense_layers: 
    - units: 256
      activation: "relu"
      kernel_initializer: "he_uniform"
    - units: 128
      activation: "relu"
      kernel_initializer: "he_uniform"
    - units: 64
      activation: "relu"
      kernel_initializer: "he_uniform"
compile:
  loss:
    function: "MeanSquaredError"
  optimizer:
    classname: "Adam"
    learning_rate: 0.001
  metrics:
    - class: "MeanAbsoluteError"
      params:
        name: "mae"
    - class: "TestMetric"
      params:
        name: "Test_Metric"
    - class: "MeanQValues"
      params:
        name: "MeanQValues"
        predicted: True
    # - class: "RootMeanSquaredError"
    #   params:
    #     name: "root_mse"
    # - class: "MeanAbsolutePercentageError"
    #   params:
    #     name: "mape"
    # - class: "R2Score"
    #   params:
    #     name: "r2_score"
policy:
  classname: "epsilon"
  epsilon: 1.0
  epsilon-min: 0.001
  epsilon-decay: 0.999
replay-buffer:
  classname: "standard"
  capacity: 2000
agent:
  classname: 'DQNAgent'
  gamma: 0.95
  batch-size: 64
  train-start: 1000
train:
  classname: 'DQNTrainer'
  num-episodes: 150