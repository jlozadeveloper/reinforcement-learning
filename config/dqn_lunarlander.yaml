trainer:
  classname: 'DQNTrainer'
environment:
  id: "LunarLander-v3"
  render_mode: 'rgb_array'
  # reward_threshold: 995
model:
  classname: "mpl"
  dense_layers: 
    - units: 64
      activation: "relu"
      # kernel_initializer: "he_uniform"
    - units: 64
      activation: "relu"
      # kernel_initializer: "he_uniform"
    - units: 64
      activation: "relu"
      # kernel_initializer: "he_uniform"
compile:
  loss:
    function: "Huber"
  optimizer:
    classname: "Adam"
    learning_rate: 0.001
  # metrics:
  #   - class: "MeanSquaredError"
  #     params:
  #       name: "mse"
  #   - class: "MeanAbsoluteError"
  #     params:
  #       name: "mae"
  #   - class: "RootMeanSquaredError"
  #     params:
  #       name: "root_mse"
  #   - class: "MeanAbsolutePercentageError"
  #     params:
  #       name: "mape"
  #   - class: "R2Score"
  #     params:
  #       name: "r2_score"
policy:
  classname: "epsilon"
  update-frequency: "episode"
  epsilon: 1.0
  epsilon-min: 0.01
  epsilon-decay: 0.995
replay-buffer:
  classname: "standard"
  capacity: 2000
agent:
  classname: 'DQNAgent'
  episodes: 1000
  gamma: 0.95
  batch-size: 64
  train-start: 1500
graphs:
  - title: Score
    figsize: [20, 15]
    xlabel: "Episodes"
    ylabel: "Values"
    plots:
      - data: "loss"
        label: "Perdida"
      - data: "mse"
        label: "Mean squared error"
      - data: "mae"
        label: "Mean absolute error"
      - data: "root_mse"
        label: "Root mean squared error"
      - data: "mape"
        label: "Mean absolute percentage error"
      - data: "r2_score"
        label: "R^2 score"
