# Configuraci√≥n de DQN
# input_shape: (4,)
# num_actions: 2
trainer:
  classname: 'DQNTrainer'
environment:
  id: "LunarLander-v3"
  # max_episode_steps: 4000
  render_mode: 'rgb_array'
  # reward_threshold: 995
model:
  classname: "dueling-mpl"
  dense_layers: 
    - units: 512
      activation: "elu"
      kernel_initializer: "he_uniform"
    - units: 256
      activation: "elu"
      kernel_initializer: "he_uniform"
    - units: 128
      activation: "elu"
      kernel_initializer: "he_uniform"
compile:
  loss:
    function: "MeanSquaredError"
  optimizer:
    classname: "Adam"
    learning_rate: 0.00025
  metrics:
    - class: "MeanAbsoluteError"
      params:
        name: "mae"
    - class: "RootMeanSquaredError"
      params:
        name: "root_mse"
    - class: "MeanAbsolutePercentageError"
      params:
        name: "mape"
    - class: "R2Score"
      params:
        name: "r2_score"
policy:
  classname: "epsilon"
  # temperature: 2
  # temperature-decay: 0.99
  # temperature-min: 0.01
  # entropy-weight: 0.95
  # num_actions: 2
  epsilon: 1.0
  epsilon-min: 0.1
  epsilon-decay: 0.9999
replay-buffer:
  classname: "prioritized"
  capacity: 2000
agent:
  classname: 'dqn'
  target-update-frequency: 10
  tau: 0.1
  gamma: 0.9
  batch-size: 64
  train-start: 1500
graphs:
  - title: Score
    figsize: [20, 15]
    xlabel: "Episodes"
    ylabel: "Values"
    plots:
      - data: "loss"
        label: "Perdida"
      - data: "mae"
        label: "Mean absolute error"
      - data: "root_mse"
        label: "Root mean squared error"
      - data: "mape"
        label: "Mean absolute percentage error"
      - data: "r2_score"
        label: "R^2 score"
